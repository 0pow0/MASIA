# --- MSRA specific parameters ---

# use epsilon greedy action selector
action_selector: "epsilon_greedy"
epsilon_start: 1.0
epsilon_finish: 0.05
epsilon_anneal_time: 50000
evaluation_epsilon: 0.0

runner: "episode"
# runner: "parallel"
# batch_size_run: 8

buffer_size: 5000

# update the target network every {} episodes
target_update_interval_or_tau: 200


obs_agent_id: True
obs_last_action: False
obs_individual_obs: False

# use the Q_Learner to train
standardise_returns: False
standardise_rewards: True

# Focus on: mac, agent, learner, mixer, use_rnn
mac: "masia_mac"
agent: "masia"
agent_output_type: "q"
learner: "masia_learner"
double_q: True
mixer: "qmix"
use_rnn: True
mixing_embed_dim: 32
hypernet_layers: 2
hypernet_embed: 64

# config for state encoder
encoder_use_rnn: True
encoder_hidden_dim: 32
ae_enc_hidden_dims: []
ae_dec_hidden_dims: []

attn_embed_dim: 16

concat_obs: True

state_repre_dim: 8
action_embed_dim: 8

state_encoder: ob_attn_ae

ob_embed_dim: 32

momentum_tau: 1

# configs about whether to use latent model learning
use_latent_model: True
use_rew_pred: True
use_momentum_encoder: True
use_residual: True
pred_len: 2
latent_model: mlp
model_hidden_dim: 64
spr_dim: 32

rl_signal: True

# config for trade-off among different losses
spr_coef: 1
rew_pred_coef: 1
repr_coef: 1

# config about encoder testing/visualization
test_encoder: False

name: "masia"

# config for robust communication
noise_env: False
noise_type: 0

# ====== Three-Phase Training Configuration ======

# Phase control
training_phase: 1  # 1: Robust Pre-training, 2: Context-Aware MVE, 3: Value-Conditional Unlearning
phase1_steps: 50000  # K_pretrain: steps for Phase 1
phase2_steps: 30000  # K_mve: steps for Phase 2
phase3_steps: 100000  # M_unlearn: steps for Phase 3

# Phase 1: Robust Pre-training with Message Dropout
message_dropout_rate: 0.3  # p_drop: probability of dropping each agent's message
save_phase1_qbase: True  # Save frozen Q_base at end of Phase 1

# Phase 2: Context-Aware Value Estimation (Attention-based MVE)
mve_hidden_dim: 64  # Hidden dimension for attention
mve_num_heads: 4  # Number of attention heads
mve_lr: 0.0005  # Learning rate for V_phi

# Phase 3: Value-Conditional Unlearning
sparsity_lambda: 1.0  # Lambda: threshold multiplier for value-based pruning
anchor_beta: 0.1  # Beta: weight for anchoring loss (keep high-value behavior stable)
mve_ema_tau: 0.01  # Tau: momentum for moving average of MVE values (mu_mve)

# Ablation flags
skip_mve: False  # Ablation: skip Phase 2 (MVE training), use pure L1 in Phase 3
skip_anchor: False  # Ablation: skip anchor loss in Phase 3 (no Q-value preservation)
l1_lambda: 0.01  # Weight for pure L1 norm (used when skip_mve=True) 