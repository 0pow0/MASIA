# --- MASIA Ablation: No MVE (Phase 2 skipped) ---
# Only Phase 1 (pre-training) and Phase 3 (unlearning with anchor + L1 norm)

# use epsilon greedy action selector
action_selector: "epsilon_greedy"
epsilon_start: 1.0
epsilon_finish: 0.05
epsilon_anneal_time: 50000
evaluation_epsilon: 0.0

runner: "episode"

buffer_size: 5000

# update the target network every {} episodes
target_update_interval_or_tau: 200


obs_agent_id: True
obs_last_action: False
obs_individual_obs: False

# use the Q_Learner to train
standardise_returns: False
standardise_rewards: True

# Focus on: mac, agent, learner, mixer, use_rnn
mac: "masia_mac"
agent: "masia"
agent_output_type: "q"
learner: "masia_learner"
double_q: True
mixer: "qmix"
use_rnn: True
mixing_embed_dim: 32
hypernet_layers: 2
hypernet_embed: 64

# config for state encoder
encoder_use_rnn: True
encoder_hidden_dim: 32
ae_enc_hidden_dims: []
ae_dec_hidden_dims: []

attn_embed_dim: 16

concat_obs: True

state_repre_dim: 8
action_embed_dim: 8

state_encoder: ob_attn_ae

ob_embed_dim: 32

momentum_tau: 1

# configs about whether to use latent model learning
use_latent_model: True
use_rew_pred: True
use_momentum_encoder: True
use_residual: True
pred_len: 2
latent_model: mlp
model_hidden_dim: 64
spr_dim: 32

rl_signal: True

# config for trade-off among different losses
spr_coef: 1
rew_pred_coef: 1
repr_coef: 1

# config about encoder testing/visualization
test_encoder: False

name: "masia_no_mve"

# config for robust communication
noise_env: False
noise_type: 0

# ====== Ablation: No MVE Training ======
# Skip phase 2 (MVE training), go directly from phase 1 to phase 3
# In phase 3: use pure L1 norm (not value-weighted) + anchor loss

skip_mve: True  # Skip phase 2 entirely

# Phase control
training_phase: 1  # 1: Robust Pre-training, 2: Context-Aware MVE (skipped), 3: Unlearning
phase1_steps: 50000  # K_pretrain: steps for Phase 1
phase2_steps: 0  # Skipped
phase3_steps: 100000  # M_unlearn: steps for Phase 3

# Phase 1: Robust Pre-training with Message Dropout
message_dropout_rate: 0.3  # p_drop: probability of dropping each agent's message
save_phase1_qbase: True  # Save frozen Q_base at end of Phase 1

# Phase 3: Unlearning with Anchor + Pure L1 Norm
l1_lambda: 0.01  # Weight for pure L1 norm of messages
anchor_beta: 0.1  # Beta: weight for anchoring loss (keep high-value behavior stable)
